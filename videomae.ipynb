{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-11T08:01:33.099360Z",
     "iopub.status.busy": "2024-07-11T08:01:33.098574Z",
     "iopub.status.idle": "2024-07-11T08:01:33.104513Z",
     "shell.execute_reply": "2024-07-11T08:01:33.103434Z",
     "shell.execute_reply.started": "2024-07-11T08:01:33.099329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "        \n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:01:34.782835Z",
     "iopub.status.busy": "2024-07-11T08:01:34.782464Z",
     "iopub.status.idle": "2024-07-11T08:01:34.787401Z",
     "shell.execute_reply": "2024-07-11T08:01:34.786272Z",
     "shell.execute_reply.started": "2024-07-11T08:01:34.782805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Disable truncation for pandas output\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:01:36.222018Z",
     "iopub.status.busy": "2024-07-11T08:01:36.221514Z",
     "iopub.status.idle": "2024-07-11T08:01:36.555634Z",
     "shell.execute_reply": "2024-07-11T08:01:36.554586Z",
     "shell.execute_reply.started": "2024-07-11T08:01:36.221981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the dataset\n",
    "folder_path = \"/kaggle/input/include\"\n",
    "\n",
    "# Lists to store video paths and labels\n",
    "video_files = []\n",
    "labels = []\n",
    "\n",
    "# Read the folder structure\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".MOV\"):  # Check for video files\n",
    "            video_path = os.path.join(root, file)\n",
    "            video_files.append(video_path)\n",
    "\n",
    "            # Extract the label from the folder name\n",
    "            # Assuming the folder names are like \"Adjectives_1of8\"\n",
    "            label = os.path.basename(root).replace('_', ' ')\n",
    "#             print(label)\n",
    "            labels.append(label.split(\" \")[-1])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"video_name\": video_files, \"tag\": labels})\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:01:47.595795Z",
     "iopub.status.busy": "2024-07-11T08:01:47.594873Z",
     "iopub.status.idle": "2024-07-11T08:01:47.599843Z",
     "shell.execute_reply": "2024-07-11T08:01:47.598899Z",
     "shell.execute_reply.started": "2024-07-11T08:01:47.595763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xdf = df[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:19.524298Z",
     "iopub.status.busy": "2024-07-11T08:02:19.523437Z",
     "iopub.status.idle": "2024-07-11T08:02:19.540973Z",
     "shell.execute_reply": "2024-07-11T08:02:19.539912Z",
     "shell.execute_reply.started": "2024-07-11T08:02:19.524257Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/2762686943.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xdf['tag'] = label_encoder.fit_transform(xdf['tag'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5138.MOV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5297.MOV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5136.MOV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_9408.MOV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9250.MOV</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9333.MOV</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9332.MOV</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5139.MOV</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5299.MOV</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                video_name  \\\n",
       "0    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV   \n",
       "1    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5138.MOV   \n",
       "2    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5297.MOV   \n",
       "3    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5136.MOV   \n",
       "4    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_9408.MOV   \n",
       "..                                                                     ...   \n",
       "95  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9250.MOV   \n",
       "96  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9333.MOV   \n",
       "97  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9332.MOV   \n",
       "98  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5139.MOV   \n",
       "99  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5299.MOV   \n",
       "\n",
       "    tag  \n",
       "0     2  \n",
       "1     2  \n",
       "2     2  \n",
       "3     2  \n",
       "4     2  \n",
       "..  ...  \n",
       "95    0  \n",
       "96    0  \n",
       "97    0  \n",
       "98    0  \n",
       "99    0  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "xdf['tag'] = label_encoder.fit_transform(xdf['tag'])\n",
    "xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:29.417231Z",
     "iopub.status.busy": "2024-07-11T08:02:29.416537Z",
     "iopub.status.idle": "2024-07-11T08:02:29.423902Z",
     "shell.execute_reply": "2024-07-11T08:02:29.422967Z",
     "shell.execute_reply.started": "2024-07-11T08:02:29.417198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:26:43.249569Z",
     "iopub.status.busy": "2024-07-11T07:26:43.249084Z",
     "iopub.status.idle": "2024-07-11T07:27:31.512094Z",
     "shell.execute_reply": "2024-07-11T07:27:31.510936Z",
     "shell.execute_reply.started": "2024-07-11T07:26:43.249529Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.30.1\n",
      "    Uninstalling accelerate-0.30.1:\n",
      "      Successfully uninstalled accelerate-0.30.1\n",
      "Successfully installed accelerate-0.32.1\n",
      "Requirement already satisfied: av in /opt/conda/lib/python3.10/site-packages (12.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytorchvideo transformers[sentencepiece] evaluate\n",
    "!pip install accelerate -U\n",
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:35.033181Z",
     "iopub.status.busy": "2024-07-11T08:02:35.032315Z",
     "iopub.status.idle": "2024-07-11T08:02:35.041004Z",
     "shell.execute_reply": "2024-07-11T08:02:35.040026Z",
     "shell.execute_reply.started": "2024-07-11T08:02:35.033141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import av\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from transformers import AutoImageProcessor, VideoMAEModel,  AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:37.489712Z",
     "iopub.status.busy": "2024-07-11T08:02:37.489048Z",
     "iopub.status.idle": "2024-07-11T08:02:37.496581Z",
     "shell.execute_reply": "2024-07-11T08:02:37.495632Z",
     "shell.execute_reply.started": "2024-07-11T08:02:37.489679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:38.901743Z",
     "iopub.status.busy": "2024-07-11T08:02:38.900870Z",
     "iopub.status.idle": "2024-07-11T08:02:38.908221Z",
     "shell.execute_reply": "2024-07-11T08:02:38.907146Z",
     "shell.execute_reply.started": "2024-07-11T08:02:38.901710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.array(np.stack([x.to_ndarray(format=\"rgb24\") for x in frames]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:40.736716Z",
     "iopub.status.busy": "2024-07-11T08:02:40.735979Z",
     "iopub.status.idle": "2024-07-11T08:02:40.742568Z",
     "shell.execute_reply": "2024-07-11T08:02:40.741517Z",
     "shell.execute_reply.started": "2024-07-11T08:02:40.736686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:48.105106Z",
     "iopub.status.busy": "2024-07-11T08:02:48.104372Z",
     "iopub.status.idle": "2024-07-11T08:02:48.110916Z",
     "shell.execute_reply": "2024-07-11T08:02:48.109960Z",
     "shell.execute_reply.started": "2024-07-11T08:02:48.105077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_image_processor_inputs(file_path, image_processor):\n",
    "    container = av.open(file_path)\n",
    "    indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "    video = read_video_pyav(container, indices)\n",
    "    inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "    \n",
    "#     video_tensor = inputs['pixel_values']\n",
    "#     if video_tensor.dim() == 4:  # If the shape is (num_frames, num_channels, height, width)\n",
    "#         video_tensor = video_tensor.unsqueeze(0) \n",
    "    return inputs\n",
    "#     return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:51.007627Z",
     "iopub.status.busy": "2024-07-11T08:02:51.006842Z",
     "iopub.status.idle": "2024-07-11T08:02:51.014266Z",
     "shell.execute_reply": "2024-07-11T08:02:51.013302Z",
     "shell.execute_reply.started": "2024-07-11T08:02:51.007595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VideoClassificationDataset(Dataset):\n",
    "    def __init__(self, root_dir, data_df, image_processor):\n",
    "        self.data = data_df.dropna()\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        video_path = self.data.iloc[index]['video_name']\n",
    "\n",
    "#         video_path = os.path.join(self.root_dir, image_path)\n",
    "\n",
    "        video_encodings = get_image_processor_inputs(video_path, image_processor).pixel_values\n",
    "#         video_encodings = get_image_processor_inputs(video_path, image_processor)\n",
    "        labels = np.array(self.data.iloc[index][\"tag\"])\n",
    "\n",
    "        return video_encodings, labels\n",
    "#         return  {\"pixel_values\": video_encodings, \"labels\": torch.tensor(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:02:53.108871Z",
     "iopub.status.busy": "2024-07-11T08:02:53.108067Z",
     "iopub.status.idle": "2024-07-11T08:02:57.465419Z",
     "shell.execute_reply": "2024-07-11T08:02:57.464580Z",
     "shell.execute_reply.started": "2024-07-11T08:02:53.108830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "video_encoder = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "# video_encoder = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base\", num_labels=train_df['tag'].nunique())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:03:42.496335Z",
     "iopub.status.busy": "2024-07-11T08:03:42.495725Z",
     "iopub.status.idle": "2024-07-11T08:03:42.506221Z",
     "shell.execute_reply": "2024-07-11T08:03:42.505375Z",
     "shell.execute_reply.started": "2024-07-11T08:03:42.496306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = VideoClassificationDataset(\"/kaggle/input/include\", train_df,  image_processor)\n",
    "test_dataset = VideoClassificationDataset(\"/kaggle/input/include\",  test_df, image_processor)\n",
    "val_dataset = VideoClassificationDataset(\"/kaggle/input/include\",  val_df, image_processor)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:54.035909Z",
     "iopub.status.busy": "2024-07-11T07:27:54.035620Z",
     "iopub.status.idle": "2024-07-11T07:27:54.041449Z",
     "shell.execute_reply": "2024-07-11T07:27:54.040507Z",
     "shell.execute_reply.started": "2024-07-11T07:27:54.035885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i in train_loader:\n",
    "#     print(i[0])\n",
    "#     print(type(i))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:03:09.595445Z",
     "iopub.status.busy": "2024-07-11T08:03:09.595064Z",
     "iopub.status.idle": "2024-07-11T08:03:09.602585Z",
     "shell.execute_reply": "2024-07-11T08:03:09.601637Z",
     "shell.execute_reply.started": "2024-07-11T08:03:09.595407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VideoClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, video_encoder):\n",
    "        super(VideoClassifier, self).__init__()\n",
    "        self.video_encoder = video_encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.video_encoder.config.hidden_size, 2048),\n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, video_features):\n",
    "        outputs = self.video_encoder(video_features)\n",
    "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:03:11.533393Z",
     "iopub.status.busy": "2024-07-11T08:03:11.533018Z",
     "iopub.status.idle": "2024-07-11T08:03:11.672490Z",
     "shell.execute_reply": "2024-07-11T08:03:11.671515Z",
     "shell.execute_reply.started": "2024-07-11T08:03:11.533364Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_labels = train_df['tag'].nunique()\n",
    "\n",
    "model = VideoClassifier(num_labels, video_encoder).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:03:46.583699Z",
     "iopub.status.busy": "2024-07-11T08:03:46.583070Z",
     "iopub.status.idle": "2024-07-11T08:03:46.593300Z",
     "shell.execute_reply": "2024-07-11T08:03:46.592396Z",
     "shell.execute_reply.started": "2024-07-11T08:03:46.583671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            video_features, labels = batch\n",
    "\n",
    "            if video_features.shape[0] == 1:\n",
    "              video_features = video_features.squeeze(0)\n",
    "            else:\n",
    "              video_features = video_features.squeeze()\n",
    "\n",
    "            video_features = video_features.to(device)\n",
    "\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(video_features)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 8 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
    "                train_loss = 0.0\n",
    "\n",
    "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "        print(\"========================================================================================\")\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, best_accuracy: {best_accuracy:.4f}')\n",
    "        print(\"========================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:03:48.121367Z",
     "iopub.status.busy": "2024-07-11T08:03:48.120996Z",
     "iopub.status.idle": "2024-07-11T08:03:48.131324Z",
     "shell.execute_reply": "2024-07-11T08:03:48.130354Z",
     "shell.execute_reply.started": "2024-07-11T08:03:48.121337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data_loader,  device):\n",
    "\n",
    "    print(\"evaluate started\")\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(data_loader):\n",
    "\n",
    "          video_features, labels = batch\n",
    "\n",
    "          if video_features.shape[0] == 1:\n",
    "\n",
    "            video_features = video_features.squeeze(0)\n",
    "          else:\n",
    "            video_features = video_features.squeeze()\n",
    "\n",
    "          video_features = video_features.to(device)\n",
    "\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          logits = model(video_features)\n",
    "          loss = criterion(logits, labels)\n",
    "          total_loss += loss.item()\n",
    "\n",
    "          _, preds = torch.max(logits, 1)\n",
    "\n",
    "          all_labels.append(labels.cpu().numpy())\n",
    "          all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return loss, accuracy, f1, all_labels, all_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:54.989564Z",
     "iopub.status.busy": "2024-07-11T07:27:54.988935Z",
     "iopub.status.idle": "2024-07-11T07:27:54.999882Z",
     "shell.execute_reply": "2024-07-11T07:27:54.998956Z",
     "shell.execute_reply.started": "2024-07-11T07:27:54.989528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:03:50.513263Z",
     "iopub.status.busy": "2024-07-11T08:03:50.512539Z",
     "iopub.status.idle": "2024-07-11T10:04:44.241257Z",
     "shell.execute_reply": "2024-07-11T10:04:44.239721Z",
     "shell.execute_reply.started": "2024-07-11T08:03:50.513233Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 8/196, Train Loss: 5.5798\n",
      "Epoch 1/5, Batch 16/196, Train Loss: 5.5555\n",
      "Epoch 1/5, Batch 24/196, Train Loss: 5.5621\n",
      "Epoch 1/5, Batch 32/196, Train Loss: 5.5607\n",
      "Epoch 1/5, Batch 40/196, Train Loss: 5.5480\n",
      "Epoch 1/5, Batch 48/196, Train Loss: 5.5605\n",
      "Epoch 1/5, Batch 56/196, Train Loss: 5.5402\n",
      "Epoch 1/5, Batch 64/196, Train Loss: 5.5424\n",
      "Epoch 1/5, Batch 72/196, Train Loss: 5.5427\n",
      "Epoch 1/5, Batch 80/196, Train Loss: 5.5505\n",
      "Epoch 1/5, Batch 88/196, Train Loss: 5.5521\n",
      "Epoch 1/5, Batch 96/196, Train Loss: 5.5387\n",
      "Epoch 1/5, Batch 104/196, Train Loss: 5.5437\n",
      "Epoch 1/5, Batch 112/196, Train Loss: 5.5295\n",
      "Epoch 1/5, Batch 120/196, Train Loss: 5.5185\n",
      "Epoch 1/5, Batch 128/196, Train Loss: 5.5356\n",
      "Epoch 1/5, Batch 136/196, Train Loss: 5.5527\n",
      "Epoch 1/5, Batch 144/196, Train Loss: 5.4868\n",
      "Epoch 1/5, Batch 152/196, Train Loss: 5.4777\n",
      "Epoch 1/5, Batch 160/196, Train Loss: 5.5161\n",
      "Epoch 1/5, Batch 168/196, Train Loss: 5.4501\n",
      "Epoch 1/5, Batch 176/196, Train Loss: 5.4738\n",
      "Epoch 1/5, Batch 184/196, Train Loss: 5.5291\n",
      "Epoch 1/5, Batch 192/196, Train Loss: 5.4244\n",
      "evaluate started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Epoch 1/5, Val Loss: 5.4357, Val Accuracy: 0.0254, Val F1: 0.0014, best_accuracy: 0.0254\n",
      "========================================================================================\n",
      "Epoch 2/5, Batch 8/196, Train Loss: 5.3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "video_encoder.gradient_checkpointing_enable()\n",
    "num_epochs = 5\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3852662,
     "sourceId": 6677809,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5244703,
     "sourceId": 8736592,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5260008,
     "sourceId": 8755923,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
