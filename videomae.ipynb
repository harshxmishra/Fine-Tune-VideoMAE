{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6677809,"sourceType":"datasetVersion","datasetId":3852662},{"sourceId":8736592,"sourceType":"datasetVersion","datasetId":5244703},{"sourceId":8755923,"sourceType":"datasetVersion","datasetId":5260008}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-11T08:01:33.098574Z","iopub.execute_input":"2024-07-11T08:01:33.099360Z","iopub.status.idle":"2024-07-11T08:01:33.104513Z","shell.execute_reply.started":"2024-07-11T08:01:33.099329Z","shell.execute_reply":"2024-07-11T08:01:33.103434Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Disable truncation for pandas output\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:34.782464Z","iopub.execute_input":"2024-07-11T08:01:34.782835Z","iopub.status.idle":"2024-07-11T08:01:34.787401Z","shell.execute_reply.started":"2024-07-11T08:01:34.782805Z","shell.execute_reply":"2024-07-11T08:01:34.786272Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define the folder containing the dataset\nfolder_path = \"/kaggle/input/include\"\n\n# Lists to store video paths and labels\nvideo_files = []\nlabels = []\n\n# Read the folder structure\nfor root, dirs, files in os.walk(folder_path):\n    for file in files:\n        if file.endswith(\".MOV\"):  # Check for video files\n            video_path = os.path.join(root, file)\n            video_files.append(video_path)\n\n            # Extract the label from the folder name\n            # Assuming the folder names are like \"Adjectives_1of8\"\n            label = os.path.basename(root).replace('_', ' ')\n#             print(label)\n            labels.append(label.split(\" \")[-1])\n\n# Create a DataFrame\ndf = pd.DataFrame({\"video_name\": video_files, \"tag\": labels})\n\n# Display the DataFrame\n# print(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:36.221514Z","iopub.execute_input":"2024-07-11T08:01:36.222018Z","iopub.status.idle":"2024-07-11T08:01:36.555634Z","shell.execute_reply.started":"2024-07-11T08:01:36.221981Z","shell.execute_reply":"2024-07-11T08:01:36.554586Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"xdf = df[0:100]","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:47.594873Z","iopub.execute_input":"2024-07-11T08:01:47.595795Z","iopub.status.idle":"2024-07-11T08:01:47.599843Z","shell.execute_reply.started":"2024-07-11T08:01:47.595763Z","shell.execute_reply":"2024-07-11T08:01:47.598899Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nxdf['tag'] = label_encoder.fit_transform(xdf['tag'])\nxdf","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:19.523437Z","iopub.execute_input":"2024-07-11T08:02:19.524298Z","iopub.status.idle":"2024-07-11T08:02:19.540973Z","shell.execute_reply.started":"2024-07-11T08:02:19.524257Z","shell.execute_reply":"2024-07-11T08:02:19.539912Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2762686943.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  xdf['tag'] = label_encoder.fit_transform(xdf['tag'])\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                                                                video_name  \\\n0    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV   \n1    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5138.MOV   \n2    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5297.MOV   \n3    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5136.MOV   \n4    /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_9408.MOV   \n..                                                                     ...   \n95  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9250.MOV   \n96  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9333.MOV   \n97  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9332.MOV   \n98  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5139.MOV   \n99  /kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5299.MOV   \n\n    tag  \n0     2  \n1     2  \n2     2  \n3     2  \n4     2  \n..  ...  \n95    0  \n96    0  \n97    0  \n98    0  \n99    0  \n\n[100 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_name</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5138.MOV</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5297.MOV</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5136.MOV</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_9408.MOV</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9250.MOV</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9333.MOV</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9332.MOV</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5139.MOV</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5299.MOV</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:29.416537Z","iopub.execute_input":"2024-07-11T08:02:29.417231Z","iopub.status.idle":"2024-07-11T08:02:29.423902Z","shell.execute_reply.started":"2024-07-11T08:02:29.417198Z","shell.execute_reply":"2024-07-11T08:02:29.422967Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"!pip install -q pytorchvideo transformers[sentencepiece] evaluate\n!pip install accelerate -U\n!pip install av","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:26:43.249084Z","iopub.execute_input":"2024-07-11T07:26:43.249569Z","iopub.status.idle":"2024-07-11T07:27:31.512094Z","shell.execute_reply.started":"2024-07-11T07:26:43.249529Z","shell.execute_reply":"2024-07-11T07:27:31.510936Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\nSuccessfully installed accelerate-0.32.1\nRequirement already satisfied: av in /opt/conda/lib/python3.10/site-packages (12.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import av\n# import datasets\n# from datasets import load_dataset, DatasetDict,  Audio\nimport pandas as pd\nimport os\nimport glob\nimport io\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report, accuracy_score\nfrom transformers import AutoImageProcessor, VideoMAEModel,  AdamW\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom sklearn.metrics import f1_score, classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:35.032315Z","iopub.execute_input":"2024-07-11T08:02:35.033181Z","iopub.status.idle":"2024-07-11T08:02:35.041004Z","shell.execute_reply.started":"2024-07-11T08:02:35.033141Z","shell.execute_reply":"2024-07-11T08:02:35.040026Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:37.489048Z","iopub.execute_input":"2024-07-11T08:02:37.489712Z","iopub.status.idle":"2024-07-11T08:02:37.496581Z","shell.execute_reply.started":"2024-07-11T08:02:37.489679Z","shell.execute_reply":"2024-07-11T08:02:37.495632Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.array(np.stack([x.to_ndarray(format=\"rgb24\") for x in frames]))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:38.900870Z","iopub.execute_input":"2024-07-11T08:02:38.901743Z","iopub.status.idle":"2024-07-11T08:02:38.908221Z","shell.execute_reply.started":"2024-07-11T08:02:38.901710Z","shell.execute_reply":"2024-07-11T08:02:38.907146Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        clip_len (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    converted_len = int(clip_len * frame_sample_rate)\n    end_idx = np.random.randint(converted_len, seg_len)\n    start_idx = end_idx - converted_len\n    indices = np.linspace(start_idx, end_idx, num=clip_len)\n    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    return indices\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:40.735979Z","iopub.execute_input":"2024-07-11T08:02:40.736716Z","iopub.status.idle":"2024-07-11T08:02:40.742568Z","shell.execute_reply.started":"2024-07-11T08:02:40.736686Z","shell.execute_reply":"2024-07-11T08:02:40.741517Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def get_image_processor_inputs(file_path, image_processor):\n    container = av.open(file_path)\n    indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n    video = read_video_pyav(container, indices)\n    inputs = image_processor(list(video), return_tensors=\"pt\")\n    \n#     video_tensor = inputs['pixel_values']\n#     if video_tensor.dim() == 4:  # If the shape is (num_frames, num_channels, height, width)\n#         video_tensor = video_tensor.unsqueeze(0) \n    return inputs\n#     return video_tensor","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:48.104372Z","iopub.execute_input":"2024-07-11T08:02:48.105106Z","iopub.status.idle":"2024-07-11T08:02:48.110916Z","shell.execute_reply.started":"2024-07-11T08:02:48.105077Z","shell.execute_reply":"2024-07-11T08:02:48.109960Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# t = VideoClassificationDataset(\"/kaggle/input/include\", train_df,  image_processor).__getitem__(0)\n# t[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:27:47.560605Z","iopub.execute_input":"2024-07-11T07:27:47.561457Z","iopub.status.idle":"2024-07-11T07:27:47.569159Z","shell.execute_reply.started":"2024-07-11T07:27:47.561424Z","shell.execute_reply":"2024-07-11T07:27:47.568297Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class VideoClassificationDataset(Dataset):\n    def __init__(self, root_dir, data_df, image_processor):\n        self.data = data_df.dropna()\n        self.root_dir = root_dir\n        self.image_processor = image_processor\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n\n        video_path = self.data.iloc[index]['video_name']\n\n#         video_path = os.path.join(self.root_dir, image_path)\n\n        video_encodings = get_image_processor_inputs(video_path, image_processor).pixel_values\n#         video_encodings = get_image_processor_inputs(video_path, image_processor)\n        labels = np.array(self.data.iloc[index][\"tag\"])\n\n        return video_encodings, labels\n#         return  {\"pixel_values\": video_encodings, \"labels\": torch.tensor(labels)}","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:51.006842Z","iopub.execute_input":"2024-07-11T08:02:51.007627Z","iopub.status.idle":"2024-07-11T08:02:51.014266Z","shell.execute_reply.started":"2024-07-11T08:02:51.007595Z","shell.execute_reply":"2024-07-11T08:02:51.013302Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\nimage_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\nvideo_encoder = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n# video_encoder = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base\", num_labels=train_df['tag'].nunique())\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:02:53.108067Z","iopub.execute_input":"2024-07-11T08:02:53.108871Z","iopub.status.idle":"2024-07-11T08:02:57.465419Z","shell.execute_reply.started":"2024-07-11T08:02:53.108830Z","shell.execute_reply":"2024-07-11T08:02:57.464580Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"train_dataset = VideoClassificationDataset(\"/kaggle/input/include\", train_df,  image_processor)\ntest_dataset = VideoClassificationDataset(\"/kaggle/input/include\",  test_df, image_processor)\nval_dataset = VideoClassificationDataset(\"/kaggle/input/include\",  val_df, image_processor)\n\nbatch_size = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:42.495725Z","iopub.execute_input":"2024-07-11T08:03:42.496335Z","iopub.status.idle":"2024-07-11T08:03:42.506221Z","shell.execute_reply.started":"2024-07-11T08:03:42.496306Z","shell.execute_reply":"2024-07-11T08:03:42.505375Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# for i in train_loader:\n#     print(i[0])\n#     print(type(i))\n#     break","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:27:54.035620Z","iopub.execute_input":"2024-07-11T07:27:54.035909Z","iopub.status.idle":"2024-07-11T07:27:54.041449Z","shell.execute_reply.started":"2024-07-11T07:27:54.035885Z","shell.execute_reply":"2024-07-11T07:27:54.040507Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class VideoClassifier(nn.Module):\n    def __init__(self, num_labels, video_encoder):\n        super(VideoClassifier, self).__init__()\n        self.video_encoder = video_encoder\n        self.classifier = nn.Sequential(\n            nn.Linear(self.video_encoder.config.hidden_size, 2048),\n            nn.ReLU(), \n            \n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, num_labels)\n        )\n\n    def forward(self, video_features):\n        outputs = self.video_encoder(video_features)\n        pooled_output = outputs['last_hidden_state'][:, 0, :]\n        logits = self.classifier(pooled_output)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:09.595064Z","iopub.execute_input":"2024-07-11T08:03:09.595445Z","iopub.status.idle":"2024-07-11T08:03:09.602585Z","shell.execute_reply.started":"2024-07-11T08:03:09.595407Z","shell.execute_reply":"2024-07-11T08:03:09.601637Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"num_labels = train_df['tag'].nunique()\n\nmodel = VideoClassifier(num_labels, video_encoder).to(device)\noptimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:11.533018Z","iopub.execute_input":"2024-07-11T08:03:11.533393Z","iopub.status.idle":"2024-07-11T08:03:11.672490Z","shell.execute_reply.started":"2024-07-11T08:03:11.533364Z","shell.execute_reply":"2024-07-11T08:03:11.671515Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# from typing import List, Union\n# import torch\n# from transformers import AutoImageProcessor\n\n# class CustomDataCollator:\n#     def __init__(self, image_processor: AutoImageProcessor):\n#         self.image_processor = image_processor\n\n#     def __call__(self, batch):\n#         video_encodings, labels = zip(*batch)\n\n#         # Assuming video_encodings contains batches of videos as tensors\n#         # For demonstration, assuming video_encodings are already in the correct tensor format\n#         video_encodings_stacked = torch.stack(video_encodings, dim=0)\n\n#         # Process video encodings using image_processor (tokenizer)\n#         processed_inputs = self.image_processor(list(video_encodings_stacked), return_tensors=\"pt\")\n\n#         return {\n#             \"input_ids\": processed_inputs.input_ids,\n#             \"pixel_values\": processed_inputs.pixel_values,\n#             \"labels\": torch.tensor(labels),\n#         }\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:27:54.942543Z","iopub.execute_input":"2024-07-11T07:27:54.942834Z","iopub.status.idle":"2024-07-11T07:27:54.948058Z","shell.execute_reply.started":"2024-07-11T07:27:54.942810Z","shell.execute_reply":"2024-07-11T07:27:54.947194Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# from transformers import TrainingArguments, Trainer\n# # Training arguments\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",\n#     eval_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     learning_rate=2e-5,\n#     per_device_train_batch_size=batch_size,\n#     per_device_eval_batch_size=batch_size,\n#     num_train_epochs=5,\n#     weight_decay=0.01,\n#     logging_dir='./logs',\n#     logging_steps=10,\n#     load_best_model_at_end=True,\n#     metric_for_best_model=\"accuracy\"\n# )\n\n# # Custom metrics function\n# from sklearn.metrics import accuracy_score\n\n# def compute_metrics(p):\n#     preds = np.argmax(p.predictions, axis=1)\n#     return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n# custom_collator = CustomDataCollator(image_processor)\n\n# # Trainer\n# trainer = Trainer(\n#     model=video_encoder,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     tokenizer=image_processor,\n#     compute_metrics=compute_metrics,\n#     data_collator=custom_collator\n# )\n\n# # Train the model\n# trainer.train()\n\n# # Evaluate the model\n# trainer.evaluate(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:27:54.949390Z","iopub.execute_input":"2024-07-11T07:27:54.950025Z","iopub.status.idle":"2024-07-11T07:27:54.958566Z","shell.execute_reply.started":"2024-07-11T07:27:54.949993Z","shell.execute_reply":"2024-07-11T07:27:54.957694Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\n# Define the training function\ndef train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n\n    best_accuracy = 0.0\n\n    for epoch in range(num_epochs):\n\n        model.train()\n\n        for i, batch in enumerate(train_loader):\n\n            video_features, labels = batch\n\n            if video_features.shape[0] == 1:\n              video_features = video_features.squeeze(0)\n            else:\n              video_features = video_features.squeeze()\n\n            video_features = video_features.to(device)\n\n            labels = labels.view(-1)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n\n            logits = model(video_features)\n\n            loss = criterion(logits, labels)\n            loss.backward()\n\n            optimizer.step()\n\n            if (i+1) % 8 == 0:\n                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n                train_loss = 0.0\n\n        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n\n        if val_accuracy > best_accuracy:\n            best_accuracy = val_accuracy\n            torch.save(model.state_dict(), 'best_model.pt')\n\n        print(\"========================================================================================\")\n        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, best_accuracy: {best_accuracy:.4f}')\n        print(\"========================================================================================\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:46.583070Z","iopub.execute_input":"2024-07-11T08:03:46.583699Z","iopub.status.idle":"2024-07-11T08:03:46.593300Z","shell.execute_reply.started":"2024-07-11T08:03:46.583671Z","shell.execute_reply":"2024-07-11T08:03:46.592396Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"\ndef evaluate(model, data_loader,  device):\n\n    print(\"evaluate started\")\n    all_labels = []\n    all_preds = []\n    total_loss = 0.0\n\n    with torch.no_grad():\n\n        for i, batch in enumerate(data_loader):\n\n          video_features, labels = batch\n\n          if video_features.shape[0] == 1:\n\n            video_features = video_features.squeeze(0)\n          else:\n            video_features = video_features.squeeze()\n\n          video_features = video_features.to(device)\n\n          labels = labels.view(-1)\n          labels = labels.to(device)\n\n          logits = model(video_features)\n          loss = criterion(logits, labels)\n          total_loss += loss.item()\n\n          _, preds = torch.max(logits, 1)\n\n          all_labels.append(labels.cpu().numpy())\n          all_preds.append(preds.cpu().numpy())\n\n\n    all_labels = np.concatenate(all_labels, axis=0)\n    all_preds = np.concatenate(all_preds, axis=0)\n\n    loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    return loss, accuracy, f1, all_labels, all_preds\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:48.120996Z","iopub.execute_input":"2024-07-11T08:03:48.121367Z","iopub.status.idle":"2024-07-11T08:03:48.131324Z","shell.execute_reply.started":"2024-07-11T08:03:48.121337Z","shell.execute_reply":"2024-07-11T08:03:48.130354Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.reset_max_memory_allocated()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:27:54.988935Z","iopub.execute_input":"2024-07-11T07:27:54.989564Z","iopub.status.idle":"2024-07-11T07:27:54.999882Z","shell.execute_reply.started":"2024-07-11T07:27:54.989528Z","shell.execute_reply":"2024-07-11T07:27:54.998956Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nvideo_encoder.gradient_checkpointing_enable()\nnum_epochs = 5\ntrain(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:03:50.512539Z","iopub.execute_input":"2024-07-11T08:03:50.513263Z","iopub.status.idle":"2024-07-11T10:04:44.241257Z","shell.execute_reply.started":"2024-07-11T08:03:50.513233Z","shell.execute_reply":"2024-07-11T10:04:44.239721Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch 1/5, Batch 8/196, Train Loss: 5.5798\nEpoch 1/5, Batch 16/196, Train Loss: 5.5555\nEpoch 1/5, Batch 24/196, Train Loss: 5.5621\nEpoch 1/5, Batch 32/196, Train Loss: 5.5607\nEpoch 1/5, Batch 40/196, Train Loss: 5.5480\nEpoch 1/5, Batch 48/196, Train Loss: 5.5605\nEpoch 1/5, Batch 56/196, Train Loss: 5.5402\nEpoch 1/5, Batch 64/196, Train Loss: 5.5424\nEpoch 1/5, Batch 72/196, Train Loss: 5.5427\nEpoch 1/5, Batch 80/196, Train Loss: 5.5505\nEpoch 1/5, Batch 88/196, Train Loss: 5.5521\nEpoch 1/5, Batch 96/196, Train Loss: 5.5387\nEpoch 1/5, Batch 104/196, Train Loss: 5.5437\nEpoch 1/5, Batch 112/196, Train Loss: 5.5295\nEpoch 1/5, Batch 120/196, Train Loss: 5.5185\nEpoch 1/5, Batch 128/196, Train Loss: 5.5356\nEpoch 1/5, Batch 136/196, Train Loss: 5.5527\nEpoch 1/5, Batch 144/196, Train Loss: 5.4868\nEpoch 1/5, Batch 152/196, Train Loss: 5.4777\nEpoch 1/5, Batch 160/196, Train Loss: 5.5161\nEpoch 1/5, Batch 168/196, Train Loss: 5.4501\nEpoch 1/5, Batch 176/196, Train Loss: 5.4738\nEpoch 1/5, Batch 184/196, Train Loss: 5.5291\nEpoch 1/5, Batch 192/196, Train Loss: 5.4244\nevaluate started\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"========================================================================================\nEpoch 1/5, Val Loss: 5.4357, Val Accuracy: 0.0254, Val F1: 0.0014, best_accuracy: 0.0254\n========================================================================================\nEpoch 2/5, Batch 8/196, Train Loss: 5.3618\n","output_type":"stream"},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import torch\n# import pytorchvideo\n# import pytorchvideo.data\n# from torch.utils.data import DataLoader, Dataset\n# from torchvision.transforms import Compose, Lambda, Resize, RandomHorizontalFlip, RandomCrop\n# from pytorchvideo.transforms import ApplyTransformToKey, Normalize, UniformTemporalSubsample\n# from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer\n# import evaluate\n# from sklearn.model_selection import train_test_split\n\n# ######################################################################################\n# ######################          DEFINE PATHS HERE      ###############################\n# ######################################################################################\n\n# # Path to INCLUDE dataset\n# dataset_root_path = '/kaggle/input/include/Adjectives_1of8/Adjectives'  # Replace with the actual path to your dataset\n# model_ckpt = \"MCG-NJU/videomae-base\"\n# batch_size = 8\n\n# ######################################################################################\n# ######################################################################################\n# ######################################################################################\n\n# # Load all video file paths for determining classes\n# all_video_file_paths = [os.path.join(dp, f) for dp, dn, filenames in os.walk(dataset_root_path) for f in filenames if f.endswith(\".MOV\")]\n\n# # Extract class labels and create mapping dictionaries\n# class_labels = sorted({str(path).split(\"/\")[-2] for path in all_video_file_paths})\n# label2id = {label: i for i, label in enumerate(class_labels)}\n# id2label = {i: label for label, i in label2id.items()}\n\n# # Load pre-trained image processor and model\n# image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n# model = VideoMAEForVideoClassification.from_pretrained(\n#     model_ckpt,\n#     label2id=label2id,\n#     id2label=id2label,\n#     ignore_mismatched_sizes=True  # Necessary if the model expects a different input size\n# )\n\n# # Extract necessary parameters from the image processor\n# mean = image_processor.image_mean\n# std = image_processor.image_std\n\n# # Determine resizing strategy\n# if \"shortest_edge\" in image_processor.size:\n#     resize_to = (image_processor.size[\"shortest_edge\"], image_processor.size[\"shortest_edge\"])\n# else:\n#     resize_to = (224, 224)  # Use a default size if specific sizes are not available\n\n# num_frames_to_sample = model.config.num_frames\n# sample_rate = 4\n# fps = 30\n# clip_duration = num_frames_to_sample * sample_rate / fps\n\n# # Define transformations\n# train_transform = Compose([\n#     ApplyTransformToKey(\n#         key=\"video\",\n#         transform=Compose([\n#             UniformTemporalSubsample(num_frames_to_sample),\n#             Lambda(lambda x: x / 255.0),\n#             Normalize(mean, std),\n#             RandomCrop(resize_to),\n#             RandomHorizontalFlip(p=0.5),\n#         ]),\n#     ),\n# ])\n\n# val_transform = Compose([\n#     ApplyTransformToKey(\n#         key=\"video\",\n#         transform=Compose([\n#             UniformTemporalSubsample(num_frames_to_sample),\n#             Lambda(lambda x: x / 255.0),\n#             Normalize(mean, std),\n#             Resize(resize_to),\n#         ]),\n#     ),\n# ])\n\n# # Custom dataset class to handle INCLUDE dataset's `.MOV` files\n# class SignLanguageDataset(Dataset):\n#     def __init__(self, root_dir, transform=None, clip_duration=clip_duration):\n#         self.root_dir = root_dir\n#         self.transform = transform\n#         self.clip_duration = clip_duration\n#         self.video_paths, self.labels = self._load_dataset()\n#         self.label_map = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n\n#     def _load_dataset(self):\n#         video_paths = []\n#         labels = []\n#         for root, dirs, files in os.walk(self.root_dir):\n#             for file in files:\n#                 if file.endswith(\".MOV\"):\n#                     label = os.path.basename(root)\n#                     video_paths.append(os.path.join(root, file))\n#                     labels.append(label)\n#         return video_paths, labels\n\n#     def __len__(self):\n#         return len(self.video_paths)\n\n#     def __getitem__(self, idx):\n#         video_path = self.video_paths[idx]\n#         label = self.label_map[self.labels[idx]]\n\n#         video = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_path)\n#         frames, _ = video.get_clip(0, self.clip_duration)\n        \n#         if self.transform:\n#             frames = self.transform({\"video\": frames})[\"video\"]  # Apply transformations\n\n#         return {\"video\": frames, \"label\": label}\n\n# # Instantiate dataset\n# dataset = SignLanguageDataset(dataset_root_path, transform=train_transform)\n\n# # Calculate the number of samples\n# n_samples = len(dataset)\n\n# # Adjust train_size and test_size based on the number of samples\n# train_size = int(n_samples * 0.8)  # 80% for training\n# test_size = n_samples - train_size  # Remaining for testing\n\n# # Split dataset into training and validation sets\n# train_dataset, val_dataset = train_test_split(dataset, train_size=train_size, test_size=test_size, random_state=42)\n\n# # Create DataLoaders\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# # Define metrics computation\n# metric = evaluate.load(\"accuracy\")\n\n# def compute_metrics(eval_pred):\n#     predictions = np.argmax(eval_pred.predictions, axis=1)\n#     return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n\n# # Data collation function\n# def collate_fn(examples):\n#     pixel_values = torch.stack([example[\"video\"].permute(1, 0, 2, 3) for example in examples])\n#     labels = torch.tensor([example[\"label\"] for example in examples])\n#     return {\"pixel_values\": pixel_values, \"labels\": labels}\n\n# # Set up Trainer with appropriate training arguments\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",\n#     eval_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     learning_rate=5e-5,\n#     per_device_train_batch_size=batch_size,\n#     per_device_eval_batch_size=batch_size,\n#     num_train_epochs=5,  # Modify this based on your dataset size\n#     weight_decay=0.01,\n#     load_best_model_at_end=True,\n#     metric_for_best_model=\"accuracy\",\n#     logging_dir='./logs',\n# )\n\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_loader.dataset,\n#     eval_dataset=val_loader.dataset,\n#     tokenizer=image_processor,\n#     compute_metrics=compute_metrics,\n#     data_collator=collate_fn,\n# )\n\n# # Start training\n# trainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:27.631496Z","iopub.status.idle":"2024-07-11T08:01:27.631843Z","shell.execute_reply.started":"2024-07-11T08:01:27.631670Z","shell.execute_reply":"2024-07-11T08:01:27.631684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# from shutil import copy2\n# from sklearn.model_selection import train_test_split\n\n\n# def videoprocessing(path, mp4path):\n#     for root, _, file in os.walk(path):\n#         for filename in file:\n#             if filename.endswith(\".MOV\"):\n#                 print(root.split('/')[-1])\n#                 print(root)\n#                 os.makedirs(os.path.join(mp4path, root.split('/')[-1], \"mp4\"), exist_ok=True)\n#                 base_filename, _ = os.path.splitext(filename)\n#                 output_filename = f\"{base_filename}.mp4\"\n#                 input_filepath = os.path.join(root, filename)\n#                 # os.mkdir(os.path.join(root, \"mp4\")\n#                 output_filepath = os.path.join(mp4path, root.split('/')[-1], output_filename)\n#                 os.system(f\"ffmpeg -i {input_filepath} {output_filepath}\")\n#                 print(f\"Converted {filename} to {output_filename}\")\n\n# def split_video_data(mp4folder, output_folder, train_size=0.8, val_size=0.1, test_size=0.1):\n#     all_videos = {}  # Dictionary to store videos per category\n#     for category in os.listdir(mp4folder):\n#         category_path = os.path.join(mp4folder, category)\n#         if os.path.isdir(category_path):\n#             category_videos = []\n#             for root, _, files in os.walk(category_path):\n#                 for file in files:\n\n#                     video_path = os.path.join(root, file)\n#                     category_videos.append(video_path)\n#             all_videos[category] = category_videos\n\n#     # Print the number of videos found in each category for debugging\n#     print(\"Number of videos found per category:\")\n#     for category, videos in all_videos.items():\n#         print(f\"{category}: {len(videos)}\")\n\n#     if not all_videos:\n#         raise ValueError(\"No video files found in the input folder.\")\n\n#     # Split videos within each category\n#     for category, videos in all_videos.items():\n#         train_videos, test_val_videos = train_test_split(videos, train_size=train_size, random_state=42)\n#         val_videos, test_videos = train_test_split(test_val_videos, test_size=test_size / (test_size + val_size), random_state=42)\n\n#         os.makedirs(os.path.join(output_folder, 'train', category), exist_ok=True)\n#         os.makedirs(os.path.join(output_folder, 'val', category), exist_ok=True)\n#         os.makedirs(os.path.join(output_folder, 'test', category), exist_ok=True)\n\n#         for folder, video_list in [('train', train_videos), ('val', val_videos), ('test', test_videos)]:\n#             for video in video_list:\n#                 output_path = os.path.join(output_folder, folder, category, os.path.basename(video))\n#                 copy2(video, output_path)\n\n# if __name__ == \"__main__\":\n#     input_folder = \"/kaggle/input/include/Adjectives_1of8\"  \n#     mp4path = \"/kaggle/working/\" # Dumb fucking logic, useless intermediate path for mp4s\n#     output_folder = \"'/kaggle/working/ouput'\"\n#     videoprocessing(input_folder, mp4path)\n#     split_video_data(mp4path, output_folder)\n#     # TODO: add argparse","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:27.632825Z","iopub.status.idle":"2024-07-11T08:01:27.633182Z","shell.execute_reply.started":"2024-07-11T08:01:27.633004Z","shell.execute_reply":"2024-07-11T08:01:27.633018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-07-11T08:01:27.634012Z","iopub.status.idle":"2024-07-11T08:01:27.634351Z","shell.execute_reply.started":"2024-07-11T08:01:27.634190Z","shell.execute_reply":"2024-07-11T08:01:27.634204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###FINETUNING WITH NUMPY ARRAY","metadata":{}},{"cell_type":"markdown","source":"DIRECTLY FINETUNING WITH VIDEOS","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}